import torch
import numpy as np
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import os
import sys
import warnings

warnings.filterwarnings("ignore", message="A parameter name that contains `beta`"
                                          "e will be renamed internally to `bias`.")
warnings.filterwarnings("ignore", message="A parameter name that contains `gamma` "
                                          "will be renamed internally to `weight`.")

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

file_path = '/Users/melihsinancubukcuoglu/Desktop/output.txt'
try:
    with open(file_path, 'r') as file:
        lines = file.readlines()
except FileNotFoundError:
    print(f"Error: The file {file_path} does not exist.")
    sys.exit(1)

print("File path opened successfully.")

gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', clean_up_tokenization_spaces=True)
gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')

print("Models and tokenizers initialized successfully.")


def search(query):
    query = query.lower()
    relevant_logs = [line.strip() for line in lines if query in line.lower()]
    return relevant_logs


def generate_response(query):
    retrieved_logs = search(query)
    if not retrieved_logs:
        return "No relevant logs found."

    logs_text = "\n".join(retrieved_logs)
    prompt = f"Here is the relevant log information:\n\n{logs_text}\n\nBased on the log information, please provide a concise answer to the query: '{query}'."

    inputs = gpt_tokenizer(prompt, return_tensors='pt')
    outputs = gpt_model.generate(**inputs, max_length=2000, num_return_sequences=1, pad_token_id=gpt_tokenizer.eos_token_id, temperature=0.7, top_p=0.9)

    response = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.strip()

def chatbot():
    print("Welcome to the log analysis chatbot!")
    print("You can ask questions about the logs, or type 'exit' to quit.")
    while True:
        query = input("Ask a question: ")
        retrieved_logs = retrieve_similar_logs(query, vectorizer, index, data)
        if query.lower() in ['exit', 'quit', 'bye']:
            print("Exiting the chatbot. Goodbye!")
            break
        answer = generate_answer(query, retrieved_logs)
        print("Model Response:\n", answer)


chatbot()